# MiniCluster Kubernetes â€” Simulation IAâ€“HPC
**Auteur : Samuel Duarte Dos Santos**

---

## ğŸ¯ Objectif du projet

Ce projet a pour but de construire un **mini-cluster Kubernetes local**, configurÃ© entiÃ¨rement Ã  la main, afin de simuler un environnement HPC (High Performance Computing) sur un poste personnel.  
Lâ€™objectif principal est de dÃ©montrer la comprÃ©hension et la maÃ®trise des concepts fondamentaux de lâ€™orchestration de conteneurs, du rÃ©seau distribuÃ© et du stockage partagÃ©, dans une logique de mise en place dâ€™un environnement dâ€™exÃ©cution pour des workloads IA.

---

## ğŸ§± Architecture du cluster

Lâ€™infrastructure est composÃ©e de trois machines virtuelles Ubuntu 22.04 :

- **1 master node** : gÃ¨re le plan de contrÃ´le et lâ€™orchestration du cluster.  
- **2 worker nodes** : exÃ©cutent les pods et workloads applicatifs.  
- **1 volume partagÃ© NFS** : simule un systÃ¨me de fichiers distribuÃ© de type HPC (analogue Ã  Lustre).

Chaque machine virtuelle dispose de 2 vCPU, 2 Go de RAM et 10 Go dâ€™espace disque.  
Elles sont reliÃ©es sur un rÃ©seau interne commun via VirtualBox ou Multipass.

Lâ€™ensemble repose sur les composants suivants :
- **Kubernetes v1.30.14** installÃ© manuellement via `kubeadm`
- **Containerd** comme runtime de conteneurs
- **Flannel** comme plugin rÃ©seau (CNI)
- **NFS** comme solution de stockage partagÃ©

---

## âš™ï¸ Processus de mise en place

Le dÃ©ploiement a Ã©tÃ© rÃ©alisÃ© entiÃ¨rement manuellement, Ã©tape par Ã©tape :

1. **CrÃ©ation des trois machines virtuelles** : un master et deux workers, tous basÃ©s sur Ubuntu 22.04.
2. **Installation des composants Kubernetes** : kubeadm, kubelet, kubectl, containerd et configuration systÃ¨me (dÃ©sactivation du swap, activation du forwarding IP et des modules noyau).
3. **Initialisation du cluster sur le master node** Ã  lâ€™aide de `kubeadm init` avec une plage rÃ©seau adaptÃ©e pour Flannel.
4. **DÃ©ploiement du rÃ©seau de pods** en appliquant la configuration officielle de Flannel.
5. **Jointure des deux workers** au cluster grÃ¢ce Ã  la commande `kubeadm join` gÃ©nÃ©rÃ©e automatiquement.
6. **VÃ©rification du cluster** avec `kubectl get nodes`, confirmant la prÃ©sence et lâ€™Ã©tat des trois nÅ“uds.
7. **Mise en place dâ€™un stockage partagÃ© NFS**, montÃ© sur tous les nÅ“uds, pour simuler le fonctionnement dâ€™un systÃ¨me de fichiers distribuÃ© utilisÃ© dans les environnements HPC.
8. **DÃ©ploiement dâ€™un pod de test Nginx** pour valider le bon fonctionnement du rÃ©seau et la rÃ©partition des charges entre les workers.
9. **VÃ©rification du fonctionnement** : Ã©tat â€œReadyâ€ des nÅ“uds, pod en exÃ©cution, volume partagÃ© fonctionnel sur lâ€™ensemble du cluster.

---

## ğŸ“Š RÃ©sultats obtenus

Ã€ la fin du dÃ©ploiement :

- Le cluster Kubernetes est **entiÃ¨rement fonctionnel**, avec un master et deux workers en Ã©tat `Ready`.
- Le **rÃ©seau Flannel** permet la communication inter-pods sur lâ€™ensemble du cluster.
- Le **stockage partagÃ© NFS** est accessible depuis les trois machines, validant la simulation dâ€™un environnement HPC distribuÃ©.
- Un **pod Nginx** dÃ©ployÃ© et accessible confirme le bon fonctionnement de lâ€™orchestration.

Ces rÃ©sultats dÃ©montrent la capacitÃ© Ã  configurer et faire fonctionner un environnement Kubernetes sans outils simplifiÃ©s (pas de Minikube, pas de k3s), en comprenant chaque composant systÃ¨me impliquÃ©.

---

## ğŸ” Aspects techniques maÃ®trisÃ©s

- Installation manuelle complÃ¨te de Kubernetes via `kubeadm`
- Configuration du runtime **containerd** avec `SystemdCgroup`
- Gestion des modules noyau (`br_netfilter`, `overlay`) et paramÃ¨tres `sysctl`
- DÃ©ploiement dâ€™un **CNI Flannel** pour la connectivitÃ© des pods
- Mise en place dâ€™un **stockage distribuÃ© NFS** simulant un systÃ¨me de fichiers HPC
- Diagnostic rÃ©seau et rÃ©solution dâ€™erreurs (`NotReady`, swap, IP forwarding)
- Structure et vÃ©rification multi-nÅ“uds Ã  lâ€™aide de `kubectl`
- Conception dâ€™un environnement reproductible et extensible pour futurs tests IAâ€“HPC

---

## ğŸ§  Enseignements clÃ©s

Ce projet mâ€™a permis de renforcer plusieurs compÃ©tences essentielles :

- **Approche bas-niveau de Kubernetes** : comprendre et manipuler kubeadm, kubelet et containerd sans outils dâ€™abstraction.  
- **Infrastructure distribuÃ©e** : interconnexion rÃ©seau, communication inter-pods, gestion du control-plane.  
- **Gestion du stockage HPC-like** : configuration et test dâ€™un partage de fichiers entre nÅ“uds.  
- **RÃ©solution de problÃ¨mes systÃ¨me** : dÃ©bogage du kubelet, configuration des cgroups et du kernel, suivi des logs et vÃ©rification des Ã©tats.  
- **PrÃ©paration Ã  lâ€™intÃ©gration IAâ€“HPC** : base technique pour relier Slurm, Pyxis ou Podman afin dâ€™exÃ©cuter des workloads IA dans des conteneurs sur ce cluster.

---

## ğŸš€ Perspectives dâ€™Ã©volution

Ce mini-cluster servira de socle Ã  plusieurs extensions :

- IntÃ©gration de **Slurm** avec **Pyxis** ou **Slinky** pour exÃ©cuter des jobs IA via des conteneurs Podman.  
- Ajout dâ€™un **GPU runtime** (NVIDIA Container Toolkit) pour tester des workloads IA distribuÃ©s.  
- Mise en place dâ€™une **surveillance Prometheus + Grafana** pour visualiser lâ€™utilisation CPU/GPU/mÃ©moire.  
- Automatisation complÃ¨te du dÃ©ploiement avec **Ansible** ou **Terraform**.  
- Exploration de lâ€™orchestration multi-nÅ“uds via Kubernetes pour des **expÃ©riences IA parallÃ©lisÃ©es**.

---

